{% include 'autogenerated_note_code.jinja' %}


from transformers import AutoTokenizer
import numpy as np


def get_model_shape_and_input_types(model):
    datatype_map = {
        'i32': np.int32,
        'i64': np.int64,
    }
    shapes = {
        parameter.get_friendly_name(): [
            int(str(dimension)) for dimension in parameter.get_partial_shape()
        ]
        for parameter in model.get_parameters()
    }
    data_types = [datatype_map[parameter.get_element_type().get_type_name()] for parameter in model.get_parameters()]
    return shapes, data_types

def get_tokenizer_kwargs(model_shape):
    return {
            "max_length": next(iter(model_shape.values()))[1],  # Layout is NC - (batch_size, sequence_length)
            "return_token_type_ids": len(model_shape) == 3,
            "padding": "max_length",
            "return_tensors": "np",
        }


tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

shape, data_types = get_model_shape_and_input_types(model)
tokenizer_kwargs = get_tokenizer_kwargs(shape)
