"""
 OpenVINO DL Workbench
 Class for ORM model described an Infer Job

 Copyright (c) 2018 Intel Corporation

 LEGAL NOTICE: Your use of this software and any required dependent software (the “Software Package”) is subject to
 the terms and conditions of the software license agreements for Software Package, which may also include
 notices, disclaimers, or license terms for third party or open source software
 included in or with the Software Package, and your use indicates your acceptance of all such terms.
 Please refer to the “third-party-programs.txt” or other similarly-named text file included with the Software Package
 for additional details.
 You may obtain a copy of the License at
      https://software.intel.com/content/dam/develop/external/us/en/documents/intel-openvino-license-agreements.pdf
"""
from pathlib import Path
from typing import List, Optional

from sqlalchemy import Column, Integer, ForeignKey, DateTime, Boolean

from config.constants import PROFILING_ARTIFACTS_REPORT_DIR, JOBS_SCRIPTS_FOLDER_NAME, \
    PROFILING_BINARY_DATASET_FOLDER, PROFILING_CONFIGURATION_FILE_NAME, JOB_SCRIPT_NAME
from wb.main.enumerates import JobTypesEnum, StatusEnum, ModelDomainEnum
from wb.main.models import DatasetsModel
from wb.main.models.jobs_model import JobsModel
from wb.main.utils.utils import find_by_ext


class ProfilingJobModel(JobsModel):
    __tablename__ = 'profiling_jobs'

    __mapper_args__ = {
        'polymorphic_identity': JobTypesEnum.profiling_type.value
    }

    job_id = Column(Integer, ForeignKey(JobsModel.job_id), primary_key=True)

    inference_time = Column(Integer, nullable=False)
    num_single_inferences = Column(Integer, nullable=False)

    started_timestamp = Column(DateTime, nullable=True)

    autogenerated = Column(Boolean, default=False)

    # Annotations
    profiling_results: Optional[List['SingleInferenceInfoModel']]

    def __init__(self, data):
        super().__init__(data)
        self.inference_time = data['inferenceTime']
        self.num_single_inferences = data['numSingleInferences']
        self.progress = 0
        self.autogenerated = False

    def profiling_results_json(self):
        return [r.short_json() for r in self.profiling_results]

    @property
    def xml_model_path(self) -> str:
        return find_by_ext(self.project.topology.path, 'xml')

    @property
    def next_jobs(self) -> List['JobsModel']:
        profiling_results = self.profiling_results
        queued_profiling_results = [j for j in profiling_results if j.status == StatusEnum.queued]
        return [*queued_profiling_results, *super().next_jobs]

    def set_data_generation_method(self) -> None:
        self.autogenerated = self.project.topology.tokenizer_model is None

    @property
    def input_data_path(self) -> Optional[str]:
        dataset: DatasetsModel = self.project.dataset
        if self.autogenerated and self.project.topology.domain is ModelDomainEnum.NLP:
            return None
        if self.autogenerated:
            return dataset.dataset_data_dir
        # Return binary data generated for profiling
        return str(self.binary_dataset_directory_path)

    @property
    def input_data_base_dir_path(self) -> str:
        dataset: DatasetsModel = self.project.dataset
        if self.autogenerated:
            return dataset.path
        return str(self.binary_dataset_directory_path)

    # TODO 77386 Consider changing to method with optional argument for remote use case (default for local)
    @property
    def profiling_artifacts_path(self) -> Path:
        return Path(PROFILING_ARTIFACTS_REPORT_DIR) / str(self.pipeline_id)

    @property
    def profiling_scripts_dir_path(self) -> Path:
        return self.profiling_artifacts_path / JOBS_SCRIPTS_FOLDER_NAME

    @property
    def profiling_job_script_path(self) -> Path:
        return self.profiling_scripts_dir_path / JOB_SCRIPT_NAME

    @property
    def binary_dataset_directory_path(self) -> Path:
        return self.profiling_artifacts_path / PROFILING_BINARY_DATASET_FOLDER

    @property
    def configuration_file_path(self) -> Path:
        return self.profiling_scripts_dir_path / PROFILING_CONFIGURATION_FILE_NAME

    def json(self) -> dict:
        return {
            'jobId': self.job_id,
            'projectId': self.project_id,
            'originalModelId': self.project.get_top_level_model_id(),
            'type': self.get_polymorphic_job_type(),
            'deviceType': self.project.device.type,
            'inferenceTime': self.inference_time,
            'inferences': self.profiling_results_json(),
            'created': self.timestamp_to_milliseconds(self.creation_timestamp),
            'started': self.timestamp_to_milliseconds(self.started_timestamp) if self.started_timestamp else None,
            'updated': self.timestamp_to_milliseconds(self.last_modified),
            'status': self.status_to_json(),
            'autogenerated': self.autogenerated,
        }
